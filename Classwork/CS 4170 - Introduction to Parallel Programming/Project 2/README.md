Summary: Within this assignment we explored and applied the knowledge of mpi we had learned up to this point. This included many different functions and methods for parallelizing our program to achieve the best stats in speedup, efficiency, and Karp-Flatt. The program we were operating on runs the PSO algorithm, particle swarm optimization. This algorithm attempts to find the maximum by moving many different particles around constantly based on feedback they receive.

Instructions: To run this code you will need to do so on OSC by using the provided jobScript.slurm. To make changes to the algorithm for testing you will want to alter ./src/main.cpp and alter Np which is population size of the particles seeking out the highest point, Nd which is the number of present dimensions, Nt which is the number of iterations it undergoes to find the best point from all iterations, and lastly the number of trials which determines how many times each combination is run in the program, which I set at 5 to ensure timeout wasn't reached.

Algorithm: The PSO algorithm is run to find the peak of all points in a function by sending out a desired number of particles who take active feedback and move around the function accordingly until they return with their best possible answer. This function can be fine tuned to the point that it continually gives a better answer, but this would require many resources. For our PSO function we attempted to get as best results as we could without using days worth of resources. To start off we take a number of particles, Np, and we uniformly distribute them in a vector before updating the best known position and initializng velocity. Then while we remain below the number of iterations, Nt, we continue through a loop that uses dimensions and random numbers to update positions and velocities of the particles individually while also constantly updating the best known position for the individual particle as well as the entire swarm as the search continues, before finally returning the best position that was found overall.

Method: The only part of the code that was edited for parallelization was within the function PSO(). I used scatter and gather methods from MPI to parallelize my code along with boost. These methods allowed me to split up the work to best parallelize my code. I also needed to use broadcast in order to send certain values that contained the new bests, which were being tracked for the entire program, to all of the different processes. I chose to use a scatter and gather setup because these choices seemed to best fill the idea of what PSO is which is many different objects spreading out then reporting back to eachother once they had information. These methods also seemed to have the best ability in being worked into PSO of all the options we had at our disposal. I ran into many issues while programming but most of all was adapting to the new classes which had been implement. We implemented Particle and Swarm classes which were derived data types to make the data able to easily be passed and used properly in MPI. These implementations required a lot of changes to the base PSO code as well as their code themselves. I went through many iterations make small changes at a time in order to be able get the two working together.

Discussion & Analysis: A verbal explanation and review of the analysis you have performed that covers time, speedup, and efficiency.
